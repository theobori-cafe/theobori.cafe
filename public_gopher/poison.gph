[i|## title: Poisoning LLM web scrapers|/|tilde.pink|70]
[i|## date: "2025-11-01"|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|All servers available on the Internet are exposed to|/|tilde.pink|70]
[i|attacks, scraping, open port scanning, vulnerability|/|tilde.pink|70]
[i|scanning by services, DDoS, etc. What interests me today are|/|tilde.pink|70]
[i|robots that download web page content without permission.|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|I consulted the logs of my web server managed by NGINX using|/|tilde.pink|70]
[i|a visual generated by the goaccess tool. The majority of|/|tilde.pink|70]
[i|HTTP requests come from unknown web browsers and crawlers.|/|tilde.pink|70]
[i|Knowing that unidentified browsers are also crawlers. The|/|tilde.pink|70]
[i|images below show that on October 30, 2025, there were|/|tilde.pink|70]
[i|59,454 HTTP requests from crawlers.|/|tilde.pink|70]
[h|goaccess|URL:https://goaccess.io/|goaccess.io|443]
[i||/|tilde.pink|70]
[i|  /goaccess_web_browsers.png|/|tilde.pink|70]
[I|/goaccess_web_browsers.png|/~nagi/goaccess_web_browsers.png|tilde.pink|70]
[i||/|tilde.pink|70]
[i|  /goaccess_web_browsers_crawlers.png|/|tilde.pink|70]
[I|/goaccess_web_browsers_crawlers.png|/~nagi/goaccess_web_browsers_crawlers.png|tilde.pink|70]
[i||/|tilde.pink|70]
[i|Most of these crawlers have questionable ethics, and they|/|tilde.pink|70]
[i|also pollute the server's storage space and add extra|/|tilde.pink|70]
[i|workload to the processor.|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|## An aggressive solution|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|To counter these robots, I researched existing solutions. I|/|tilde.pink|70]
[i|then found a post Mastodon post that lists free, open-source|/|tilde.pink|70]
[i|tools on the tldr.nettime.org instance. The common goal of|/|tilde.pink|70]
[i|these tools is to sabotage AIs.|/|tilde.pink|70]
[h|post|URL:https://tldr.nettime.org/@asrg/113867412641585520|tldr.nettime.org|443]
[h|tldr.nettime.org|URL:https://tldr.nettime.org|tldr.nettime.org|443]
[i||/|tilde.pink|70]
[i|Among them, I chose an aggressive solution called iocaine.|/|tilde.pink|70]
[i|It is a web server that generates a page containing garbage|/|tilde.pink|70]
[i|text, and within that text there are links to new pages, and|/|tilde.pink|70]
[i|so on. It is a kind of infinite maze in which robots will|/|tilde.pink|70]
[i|get lost.|/|tilde.pink|70]
[h|iocaine|URL:https://iocaine.madhouse-project.org/|iocaine.madhouse-project.org|443]
[i||/|tilde.pink|70]
[i|## Filtering non-human visitors|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|The aim is to redirect robots to iocaine. To do this, we|/|tilde.pink|70]
[i|first need to be able to identify them. For this, I took|/|tilde.pink|70]
[i|inspiration from the ai.robots.txt project, which provides a|/|tilde.pink|70]
[i|list of IAs and robots to block. The project provides|/|tilde.pink|70]
[i|examples of NGINX configurations, which I have adapted for|/|tilde.pink|70]
[i|my needs. I also used a list of IP address ranges that|/|tilde.pink|70]
[i|correspond to Facebook web crawlers.|/|tilde.pink|70]
[h|iocaine|URL:https://iocaine.madhouse-project.org/|iocaine.madhouse-project.org|443]
[h|ai.robots.txt|URL:https://github.com/ai-robots-txt/ai.robots.txt|github.com|443]
[i||/|tilde.pink|70]
[i|I'd like to thank Agate Blue for writing an extremely|/|tilde.pink|70]
[i|detailed article on iocaine and its implementation with|/|tilde.pink|70]
[i|NGINX. I was inspired by some of his configurations.|/|tilde.pink|70]
[h|Agate Blue|URL:https://agate.blue|agate.blue|443]
[h|article|URL:https://agate.blue/2025/03/27/Pi%C3%A9ger-les-robots-d'indexation-gr%C3%A2ce-%C3%A0-nginx-et-iocaine.html#detect-robots-with-nginx|agate.blue|443]
[h|iocaine|URL:https://iocaine.madhouse-project.org/|iocaine.madhouse-project.org|443]
[i||/|tilde.pink|70]
[i|I ended up with a reverse proxy capable of redirecting non-|/|tilde.pink|70]
[i|human visitors to a desired service, i.e. iocaine. Note that|/|tilde.pink|70]
[i|in my case, NGINX runs on the host system, it is not|/|tilde.pink|70]
[i|conteuneurized, it is managed by a systemd service.|/|tilde.pink|70]
[h|iocaine|URL:https://iocaine.madhouse-project.org/|iocaine.madhouse-project.org|443]
[i||/|tilde.pink|70]
[i|## My iocaine configuration|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|I kept the basic configuration to customize the web pages|/|tilde.pink|70]
[i|generated by the tool. I've downloaded three books, Uncle|/|tilde.pink|70]
[i|Tom's cabin, Lady Chatterley's lover and Brave New World|/|tilde.pink|70]
[i|whose content will be used to generate text. The tool also|/|tilde.pink|70]
[i|needs a word list.|/|tilde.pink|70]
[h|Uncle Tom's cabin|URL:https://archive.org/details/uncletomscabinta0000stow/mode/2up?ref=ol|archive.org|443]
[h|Lady Chatterley's lover|URL:https://archive.org/details/ladychatterleysl0000dhla_o6r7/page/2/mode/2up?ref=ol|archive.org|443]
[h|Brave New World|URL:https://archive.org/details/ost-english-brave_new_world_aldous_huxley|archive.org|443]
[h|word list|URL:https://git.savannah.gnu.org/cgit/miscfiles.git/plain/web2|git.savannah.gnu.org|443]
[i||/|tilde.pink|70]
[i|## My iocaine deployment|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|To deploy the solution I chose to use Docker Compose with a|/|tilde.pink|70]
[i|configuration base suggested by the documentation. Below is|/|tilde.pink|70]
[i|the iocaine deployment file.|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|services:|/|tilde.pink|70]
[i|  iocaine:|/|tilde.pink|70]
[i|    image: git.madhouse-project.org/iocaine/iocaine:2|/|tilde.pink|70]
[i|    container_name: iocaine|/|tilde.pink|70]
[i|    ports:|/|tilde.pink|70]
[i|      - "127.0.0.1:42069:42069"|/|tilde.pink|70]
[i|      - "127.0.0.1:42070:42070"|/|tilde.pink|70]
[i|    volumes:|/|tilde.pink|70]
[i|      - "./data:/data"|/|tilde.pink|70]
[i|    environment:|/|tilde.pink|70]
[i|      - IOCAINE__SERVER__BIND="0.0.0.0:42069"|/|tilde.pink|70]
[i|      - IOCAINE__SOURCES__WORDS="/data/words.txt"|/|tilde.pink|70]
[i|      - IOCAINE__SOURCES__MARKOV=["/data/text1.txt",|/|tilde.pink|70]
[i|"/data/text2.txt", "/data/text3.txt"]|/|tilde.pink|70]
[i|      - IOCAINE__METRICS__ENABLE=true|/|tilde.pink|70]
[i|      - IOCAINE__METRICS__BIND="0.0.0.0:42070"|/|tilde.pink|70]
[i|      - IOCAINE__METRICS__LABELS=["Host","UserAgent"]|/|tilde.pink|70]
[i|    restart: unless-stopped|/|tilde.pink|70]
[i|    networks:|/|tilde.pink|70]
[i|      - monitoring_prometheus_net|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|networks:|/|tilde.pink|70]
[i|  monitoring_prometheus_net:|/|tilde.pink|70]
[i|    external: true|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|The monitoring_prometheus_net network is used by Prometheus|/|tilde.pink|70]
[i|and Prometheus exporters containers. Applying it to the|/|tilde.pink|70]
[i|iocaine container allows Pormetheus to reach the iocaine|/|tilde.pink|70]
[i|Prometheus exporter behind port 42070. In this way, we can|/|tilde.pink|70]
[i|view the metrics of iocaine via a dashboard in Grafana.|/|tilde.pink|70]
[h|iocaine|URL:https://iocaine.madhouse-project.org/|iocaine.madhouse-project.org|443]
[h|iocaine|URL:https://iocaine.madhouse-project.org/|iocaine.madhouse-project.org|443]
[i||/|tilde.pink|70]
[i|The value of IOCAINE__SOURCES__WORDS corresponds to a file|/|tilde.pink|70]
[i|which is the list of words previously downloaded and the|/|tilde.pink|70]
[i|value of IOCAINE__SOURCES__MARKOV represents a list of three|/|tilde.pink|70]
[i|files which are the aforementioned books.|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|## The robots are stuck|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|The service is now deployed and the NGINX configuration has|/|tilde.pink|70]
[i|been updated for the tool and loaded. Now all we have to do|/|tilde.pink|70]
[i|is wait to see the first trapped robots.|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|I've left iocaine running for around 20 hours already, and|/|tilde.pink|70]
[i|here's a Grafana dashboard showing some results.|/|tilde.pink|70]
[h|iocaine|URL:https://iocaine.madhouse-project.org/|iocaine.madhouse-project.org|443]
[i||/|tilde.pink|70]
[i|  /grafana_iocaine_dashboard.png|/|tilde.pink|70]
[I|/grafana_iocaine_dashboard.png|/~nagi/grafana_iocaine_dashboard.png|tilde.pink|70]
[i||/|tilde.pink|70]
[i|We can see that there have already been 128,644 requests|/|tilde.pink|70]
[i|made by robots that have landed in the labyrinth, with|/|tilde.pink|70]
[i|OpenAI and Anthropic as the main senders. Looking at the|/|tilde.pink|70]
[i|metrics, I can see that GPTBot is the least intelligent|/|tilde.pink|70]
[i|scrapper, having covered the longest path with a depth of|/|tilde.pink|70]
[i|79!|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|To get an idea of what this represents, imagine clicking 79|/|tilde.pink|70]
[i|times on a word in a text that redirects to a new text, with|/|tilde.pink|70]
[i|this word concatenated at the end of the URL.|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|## Testing with a web browser|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|To test with a web browser, I used LibreWolf and then|/|tilde.pink|70]
[i|installed an extension to change the user agent. I put|/|tilde.pink|70]
[i|gptbot in the user agent value and went to|/|tilde.pink|70]
[i|https://theobori.cafe. Here's what it looks like to be|/|tilde.pink|70]
[i|tricked.|/|tilde.pink|70]
[h|https://theobori.cafe|URL:https://theobori.cafe|theobori.cafe|443]
[i||/|tilde.pink|70]
[i|  /iocaine_trap_web_page.png|/|tilde.pink|70]
[I|/iocaine_trap_web_page.png|/~nagi/iocaine_trap_web_page.png|tilde.pink|70]
[i||/|tilde.pink|70]
[i|## Conclusion|/|tilde.pink|70]
[i||/|tilde.pink|70]
[i|I've managed to prevent LLM scrappers from stealing my|/|tilde.pink|70]
[i|content without my permission by using very little RAM and|/|tilde.pink|70]
[i|CPU. Only human visitors are allowed to download pages from|/|tilde.pink|70]
[i|my websites. This setup allows me to consume fewer resources|/|tilde.pink|70]
[i|to serve my content to IAs.|/|tilde.pink|70]
